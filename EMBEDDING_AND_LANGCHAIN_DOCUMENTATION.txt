================================================================================
EMBEDDING MODEL AND LANGCHAIN IMPLEMENTATION DOCUMENTATION
================================================================================

This document explains the embedding model used in this project, how it's 
implemented, and where LangChain is utilized throughout the codebase.

================================================================================
1. EMBEDDING MODEL
================================================================================

MODEL USED: all-MiniLM-L6-v2
LIBRARY: sentence-transformers (Hugging Face)

The all-MiniLM-L6-v2 model is a lightweight, efficient sentence transformer 
model that generates 384-dimensional embeddings. It's based on the MiniLM 
architecture and is optimized for fast inference while maintaining good 
semantic understanding.

Key Characteristics:
- Model Size: ~80MB
- Embedding Dimensions: 384
- Architecture: BERT-based with knowledge distillation
- Use Case: Semantic similarity, duplicate detection, RAG (Retrieval-Augmented Generation)
- Performance: Fast inference, good balance between speed and accuracy

================================================================================
2. EMBEDDING MODEL IMPLEMENTATION
================================================================================

LOCATION: backend/app/services/vector_service.py

The embedding model is implemented using lazy loading to optimize memory usage 
and startup time. The model is only loaded when first needed.

IMPLEMENTATION DETAILS:

1. Model Initialization (Lines 10-19):
   - Global variable `_embedding_model` stores the model instance
   - `get_embedding_model()` function implements lazy loading pattern
   - Model is loaded using: SentenceTransformer('all-MiniLM-L6-v2')
   - First call loads the model, subsequent calls return cached instance

2. Embedding Creation (Lines 22-26):
   - Function: `create_embedding(text: str) -> List[float]`
   - Takes raw text as input
   - Uses model.encode() to generate embedding vector
   - Returns 384-dimensional float list

3. Usage in Pipeline:
   - Called during receipt processing (routes.py, line 68)
   - Creates embeddings for:
     a. Duplicate detection (check_duplicates)
     b. Similarity search (find_similar_documents)
     c. RAG queries (chat_with_ledger)

4. Vector Storage:
   - Embeddings stored in MongoDB collection 'receipts'
   - Stored as array of floats in 'embedding' field
   - Used for cosine similarity calculations

5. Similarity Search Implementation:
   - Uses cosine similarity for document matching
   - Calculated as: dot_product(vec1, vec2) / (norm(vec1) * norm(vec2))
   - Threshold-based filtering (default: 0.7 for search, 0.95 for duplicates)
   - Application-level calculation (works with local MongoDB)

================================================================================
3. LANGCHAIN USAGE
================================================================================

LangChain is used for LLM orchestration and chat functionality. The following 
sections detail where and how LangChain is implemented.

3.1 LLM INITIALIZATION
LOCATION: backend/app/services/llm_orchestrator.py (Lines 17-39)

- Uses: langchain_google_genai.ChatGoogleGenerativeAI
- Model: gemini-2.5-flash (configurable via settings.LLM_MODEL)
- Fallback: gemini-1.5-pro (if primary model fails)
- Configuration:
  * Temperature: 0.1 (from settings.LLM_TEMPERATURE)
  * Max Tokens: 4096 (from settings.LLM_MAX_TOKENS)
  * API Key: From settings.GOOGLE_API_KEY

3.2 LLM ORCHESTRATION FUNCTIONS
LOCATION: backend/app/services/llm_orchestrator.py

A. validate_record() (Lines 42-126)
   Purpose: Validates extracted financial records using LLM
   LangChain Usage:
   - Uses ChatGoogleGenerativeAI instance
   - Invokes LLM with HumanMessage containing validation prompt
   - Parses JSON response from LLM
   - Returns validation result with status, issues, confidence, reasoning
   
   Features:
   - Validates completeness, consistency, format, reasonableness
   - Checks business logic
   - Provides confidence scores
   - Handles timeouts (30 seconds)

B. generate_reasoning_trace() (Lines 129-213)
   Purpose: Generates step-by-step reasoning trace for validation
   LangChain Usage:
   - Uses ChatGoogleGenerativeAI instance
   - Invokes LLM with reasoning prompt
   - Returns structured reasoning trace with steps and conclusions
   
   Features:
   - Step-by-step analysis
   - Field-by-field examination
   - Cross-field consistency checks
   - Final conclusion with confidence score

C. generate_explanation() (Lines 216-265)
   Purpose: Generates human-readable explanation of validation
   LangChain Usage:
   - Uses ChatGoogleGenerativeAI instance
   - Invokes LLM with explanation prompt
   - Returns natural language explanation
   
   Features:
   - Summary of analysis
   - Key findings
   - Issues and concerns
   - Recommendations

D. orchestrate() (Lines 268-311)
   Purpose: Main orchestration function that coordinates all LLM calls
   LangChain Usage:
   - Calls validate_record() (uses LangChain)
   - Calls generate_reasoning_trace() (uses LangChain)
   - Calls generate_explanation() (uses LangChain)
   - Combines all results into orchestration output

3.3 CHAT FUNCTIONALITY
LOCATION: backend/app/api/routes.py (Lines 268-333)

Function: chat_with_ledger()
Purpose: RAG (Retrieval-Augmented Generation) chatbot for querying ledger

LangChain Usage:
- Imports: langchain.schema.HumanMessage
- Uses: ChatGoogleGenerativeAI instance from llm_orchestrator
- Process:
  1. Creates embedding for user query (using embedding model)
  2. Finds similar documents using vector search
  3. Builds context from similar documents
  4. Constructs prompt with context and user question
  5. Invokes LLM with HumanMessage
  6. Returns LLM response with sources

Integration:
- Combines embedding model (for retrieval) with LangChain (for generation)
- Implements RAG pattern: Retrieve relevant documents, then generate answer

3.4 TRANSACTION CLASSIFICATION
LOCATION: backend/app/services/classification_service.py

Note: While classification_service.py exists, the actual classification may 
use LangChain if implemented. Check the file for LangChain usage in 
transaction categorization.

================================================================================
4. LANGCHAIN COMPONENTS USED
================================================================================

4.1 ChatGoogleGenerativeAI
   - Package: langchain-google-genai
   - Purpose: Interface to Google Gemini models
   - Usage: Primary LLM for all text generation tasks

4.2 HumanMessage
   - Package: langchain.schema
   - Purpose: Represents user input messages
   - Usage: Wrapping prompts for LLM invocation

4.3 SystemMessage (Available but not currently used)
   - Package: langchain.schema
   - Purpose: System-level instructions
   - Note: Could be used for more structured prompts

4.4 ChatPromptTemplate (Available but not currently used)
   - Package: langchain.prompts
   - Purpose: Template-based prompt construction
   - Note: Currently using string formatting, but templates available

================================================================================
5. WORKFLOW INTEGRATION
================================================================================

5.1 Receipt Processing Pipeline:
   1. OCR Extraction (no LangChain)
   2. Data Extraction (no LangChain)
   3. Embedding Creation (sentence-transformers, not LangChain)
   4. Duplicate Detection (vector similarity, not LangChain)
   5. LLM Validation (LangChain) ← Uses LangChain
   6. Reasoning Trace (LangChain) ← Uses LangChain
   7. Explanation Generation (LangChain) ← Uses LangChain
   8. Ledger Storage (no LangChain)

5.2 Chat Query Pipeline:
   1. User Query Input
   2. Query Embedding (sentence-transformers, not LangChain)
   3. Vector Search (not LangChain)
   4. Context Building
   5. LLM Response Generation (LangChain) ← Uses LangChain

================================================================================
6. CONFIGURATION
================================================================================

LLM Configuration (backend/app/core/config.py):
- LLM_MODEL: "gemini-2.5-flash" (default)
- LLM_TEMPERATURE: 0.1 (low for deterministic responses)
- LLM_MAX_TOKENS: 4096
- GOOGLE_API_KEY: Required environment variable

Embedding Model Configuration:
- Model: Hardcoded as 'all-MiniLM-L6-v2'
- No configuration needed (model downloaded automatically on first use)

================================================================================
7. DEPENDENCIES
================================================================================

Required Packages (from requirements.txt):
- langchain==0.3.27
- langchain-google-genai==2.0.10
- sentence-transformers==3.0.1
- google-generativeai==0.8.3

================================================================================
8. KEY FILES REFERENCE
================================================================================

Embedding Model:
- backend/app/services/vector_service.py
  * get_embedding_model() - Model initialization
  * create_embedding() - Embedding generation
  * find_similar_documents() - Similarity search
  * check_duplicates() - Duplicate detection

LangChain Usage:
- backend/app/services/llm_orchestrator.py
  * get_llm() - LLM initialization
  * validate_record() - Validation with LangChain
  * generate_reasoning_trace() - Reasoning with LangChain
  * generate_explanation() - Explanation with LangChain
  * orchestrate() - Main orchestration

- backend/app/api/routes.py
  * chat_with_ledger() - RAG chat with LangChain

Configuration:
- backend/app/core/config.py - LLM and model settings

================================================================================
9. SUMMARY
================================================================================

EMBEDDING MODEL:
- Model: all-MiniLM-L6-v2 (sentence-transformers)
- Purpose: Semantic similarity, duplicate detection, RAG retrieval
- Implementation: Lazy-loaded singleton in vector_service.py
- Dimensions: 384
- Storage: MongoDB vector database

LANGCHAIN:
- Primary Use: LLM orchestration for validation and chat
- Model: Google Gemini (gemini-2.5-flash)
- Key Functions:
  * Record validation
  * Reasoning trace generation
  * Explanation generation
  * RAG chat responses
- Integration: Works with embedding model for RAG pattern

The system combines:
1. Sentence Transformers (embedding model) for semantic search
2. LangChain (LLM orchestration) for intelligent validation and chat
3. MongoDB (vector storage) for document retrieval
4. MySQL (structured storage) for ledger entries

This architecture enables:
- Fast semantic search via embeddings
- Intelligent validation via LLM
- Natural language querying via RAG
- Efficient duplicate detection
- Comprehensive reasoning traces

================================================================================
END OF DOCUMENTATION
================================================================================

